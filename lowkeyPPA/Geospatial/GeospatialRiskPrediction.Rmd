---
title: 'Geospatial Risk Predictions'
author: "Trevor Kapuvari"
date: "10/18/2023"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat.explore)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)   # for KDE and ML risk class intervals
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

## Reading Data from Chicago


```{r message=FALSE, warning=FALSE}

policeDistricts <- 
  st_read("https://data.cityofchicago.org/api/geospatial/fthy-xz3r?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(District = dist_num)

drugArrest <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2017/d62x-nvdr") %>% 
    filter(Primary.Type == "NARCOTICS" & Description == "POSS: CANNABIS 30GMS OR LESS" | Description == "POSS: CANNABIS MORE THAN 30GMS") %>%
    mutate(x = gsub("[()]", "", Location)) %>%
    separate(x,into= c("Y","X"), sep=",") %>%
    mutate(X = as.numeric(X),Y = as.numeric(Y)) %>% 
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant")%>%
    st_transform('ESRI:102271') %>% 
    distinct()


chicagoBoundary <- 
  st_read("https://data.cityofchicago.org/api/geospatial/ewy2-6yfk?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') 
```
# Introduction 

We are interested in seeing Narcotic Arrests across Chicago in 2017. Narcotic arrests does not necessarily translate to drug-related crimes or occurrences such as possession or distribution, as narcotics are under reported and suffer additional selection bias in terms of racial and geographic profiling. Because the term "narcotics" has such a broad definition ranging from heroin to cannabis, we are going to look at cannabis possession exclusively. Our model will make predictions on future cannabis arrests based on previous arrests and independent variables. The variables in questions are murals, parks, and abandon buildings.


# Outcome of Interest Points 
  


with some description of what, when, and why you think selection bias may be an issue.
Plotting point data and density

> How do we analyze point data?

> Are there other geometries useful to represent point locations?

```{r fig.width=6, fig.height=4}
# uses grid.arrange to organize independent plots
grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = chicagoBoundary) +
  geom_sf(data = drugArrest, colour="blue", size=0.1, show.legend = "point") +
  labs(title= "Narcotic Arrests, Chicago 2017") +
  mapTheme(title_size = 14))

```


# Fishnet Grid of Narcotic Arrests

> What is a fishnet grid?

The `{sf}` package offers really easy way to create fishnet grids using the `st_make_grid()` function. The `cellsize` argument allows you to set the size of the grid cells; in this case it is set to `500` meters. You may have to do some research on the spatial layers projection (using `st_crs()` to know what coordinate system you are in) to understand if you are in feet or meters. If you are using Longitude and Latitude, you will need to project the data to a projected coordinate system to get distance measurements.

Examine the fishnet - the unique ID is crucial to building a data set!

```{r}
## using {sf} to create the grid
## Note the `.[chicagoBoundary] %>% ` line. This is needed to clip the grid to our data
fishnet <- 
  st_make_grid(chicagoBoundary,
               cellsize = 500, 
               square = TRUE) %>%
  .[chicagoBoundary] %>%            # fast way to select intersecting polygons
  st_sf() %>%
  mutate(uniqueID = 1:n())

```

### Aggregate points to the fishnet

> How can we aggregate points into a fishnet grid?

```{r}
## add a value of 1 to each crime, sum them with aggregate
crime_net <- 
  dplyr::select(drugArrest) %>% 
  mutate(countNarco = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countNarco = replace_na(countNarco, 0),
         uniqueID = 1:n(),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = crime_net, aes(fill = countNarco), color = NA) +
  scale_fill_viridis() +
  labs(title = "Count of Nacro Arrests for the fishnet") +
  mapTheme()

```


# A small multiple map of your risk factors in the fishnet (counts, distance and/or other feature engineering approaches).

> What features would be helpful in predicting the location of narcotics? care in the community, what indicator can show care and upbringing in community? murals? prove that murals can reate a sense of community, beautify urban spaces, express historical and cultural identity, raise awareness about social issues, and inspire creativity and wonder. Murals may decrease narcotic arrests and broken window policing. 
>
> What might these features be problematic?
>
> hint: for all the reasons we learned in class

```{r}
## only pulling a single variable for our model to keep it simple
## using Socrata again
registeredMurals <- 
  read.socrata("https://data.cityofchicago.org/Historic-Preservation/Mural-Registry/we8h-apcf") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Registered_Murals") %>%
    mutate(uniqueID = 1:n(),) %>%
    filter(uniqueID != 439) %>%
    filter(uniqueID != 440) %>%
    filter(uniqueID != 441) %>%
    filter(uniqueID != 442) %>%
    filter(uniqueID != 443) %>%
    group_by(Legend)

registeredMurals <- registeredMurals %>%
  dplyr::select(geometry, Legend)


parks <- 
  st_read("https://github.com/TrevorKap/MUSA5000/raw/main/lowkeyPPA/Geospatial/Parks%20-%20Chicago%20Park%20District%20Facilities%20(current).geojson") %>%
  st_transform(st_crs(fishnet)) 
parks <- parks %>%
  group_by(park) %>%
  rename(Legend = park) %>%
  summarise()

parks$Legend = "park"

## Neighborhoods to use in LOOCV in a bit

neighborhoods <- 
  st_read("https://raw.githubusercontent.com/blackmad/neighborhoods/master/chicago.geojson") %>%
  st_transform(st_crs(fishnet)) 

abandoned_buildings <- 
  read.socrata("https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Vacant-and-Abandoned-Building/7nii-7srd") %>%
  mutate(year = substr(date_service_request_was_received,1,4)) %>%  filter(year == "2017") %>%
  dplyr::select(Y = latitude, X = longitude) %>%
  na.omit() %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform(st_crs(fishnet)) %>%
  mutate(Legend = "Abandoned_Buildings")


streetLightsOut <- 
  read.socrata("https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Street-Lights-All-Out/zuxi-7xem") %>%
    mutate(year = substr(creation_date,1,4)) %>% filter(year == "2017") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Street_Lights_Out")

sanitation <-
  read.socrata("https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Sanitation-Code-Complaints-Hi/me59-5fac") %>%
    mutate(year = substr(creation_date,1,4)) %>% filter(year == "2017") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Sanitation")

tracts17 <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E"), 
          year = 2017, state=17, county=031, geometry=T) %>%
  st_transform('ESRI:102271')  %>% 
  dplyr::select(variable, estimate, GEOID) %>%
  spread(variable, estimate) %>%
  rename(TotalPop = B01001_001,
         NumberWhites = B01001A_001) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority_White", "Majority_Non_White")) 
```


```{r}

vars_net <- 
  rbind(registeredMurals, parks, abandoned_buildings, streetLightsOut, sanitation) %>%
  st_join(fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  left_join(fishnet, ., by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  dplyr::select(-`<NA>`) %>%
  ungroup()

```

## Nearest Neighbor & Count
> added count and nearest neighbor 

> Review: what is NN and what does `k` represent in this function?

```{r}
# convenience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid



vars_net <- vars_net %>%
    mutate(abandoned_buildings.nn = nn_function(st_c(st_coid(vars_net)), 
                                           st_c(abandoned_buildings),
                                           k = 8))


```

> What changes if we make `k` a different number?

```{r}
## Visualize the NN feature
vars_net.long.nn <- 
  dplyr::select(vars_net, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

```


```{r}
## important to drop the geometry from joining features
final_net <-
  left_join(crime_net, st_drop_geometry(vars_net), by="uniqueID") 

```

Using spatial joins to join *centroids* of fishnets to polygon for neighborhoods and districts.

> What issues arise when we try to join polygons to polygons in space?

```{r}

final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(neighborhoods, name), by = "uniqueID") %>%
    st_join(dplyr::select(policeDistricts, District), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

# for live demo
# mapview::mapview(final_net, zcol = "District")
```


```{r Maps}

ggplot() +
      geom_sf(data = vars_net.long.nn, aes(fill=value), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Nearest Neighbor of Abandon Buildings") +
      mapTheme()


ggplot() +
  geom_sf(data = vars_net, aes(fill=Registered_Murals), colour=NA) +
  scale_fill_viridis() +
  labs(title = "Count of Murals in Fishnet") +
  mapTheme()

ggplot() +
  geom_sf(data = vars_net, aes(fill=Sanitation), colour=NA) +
  scale_fill_viridis() +
  labs(title = "Sanitation Count") +
  mapTheme()

ggplot() +
  geom_sf(data = vars_net, aes(fill=Street_Lights_Out), colour=NA) +
  scale_fill_viridis() +
  labs(title = "Count of Outted Street Lights") +
  mapTheme()

ggplot() +
      geom_sf(data = final_net, aes(fill=countNarco), colour=NA) +
      scale_fill_viridis() +
      labs(title="Narcotic Arrests in Fishnet") +
      mapTheme()

```

# Local Moran's I

using {spdep} package to to build neighborhood weights and list to calculate local Moran's I.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

> What is the difference between local and global Moran's I?

A little in depth version of the chunk below can be found:

Mendez C. (2020). Spatial autocorrelation analysis in R. R Studio/RPubs. Available at <https://rpubs.com/quarcs-lab/spatial-autocorrelation>

```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

```{r}
## see ?localmoran
local_morans <- localmoran(final_net$Registered_Murals, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(Registered_Murals_Count = Registered_Murals, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```

## Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`

> What does a significant hot spot tell us about the distribution of burglaries?



```{r}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Narcotics"))
```

```{r}
# generates warning from NN
final_net <- final_net %>% 
  mutate(Mural.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(Mural.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           Mural.isSig == 1))), k = 1))

## What does k = 1 represent? 
```



# Multiple Scatterplot with Correlation
```{r Scatterplot}
final_net_nongeom <- final_net %>% st_drop_geometry()

final_net_nongeom %>%
  dplyr::select(countNarco, Registered_Murals, park, Abandoned_Buildings) %>%
  gather(Variable, Value, -countNarco) %>% 
  ggplot(aes(Value, countNarco)) +
     geom_point(size = .5) + geom_smooth(method = "lm", colour = "#FA7800") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Correlation between Narcotic Arrests and Murals") +
     plotTheme()

```

# Histogram of Dependent Varaible (Narcotic Arrests)
```{r Histogram}

  ggplot(final_net, aes(x=countNarco)) + 
  geom_histogram(color='white',fill="orange", bins=50)+
  scale_x_continuous()+
  scale_y_continuous()

```


# small multiple map of model errors by random k-fold and spatial cross validation

OK - this is a tricky bit.

Our model is actually made inside this custom function from the book called `crossValidate` that is designed to work with spatial data. 

What it does is take a `dataset`, a dependent variable `dependentVariable`, a list of independent variables `indVariables` (we feed it a list called `reg.ss.vars` here) an `id` - which is a cross validation category. It both runs a poisson model AND does a cross-validation process where it trains and tests the model on geographic holdout sets. It returns an sf object, in this case called `reg.ss.spatialCV`. 

If you want to see how it works, run the code `View(crossValidate)` to see the code behind the function.

Leave One Group Out CV on spatial features

```{r Crossvalidate, message=FALSE, warning=TRUE, include=FALSE, results='hide'}

## define the variables we want
reg.ss.vars <- c("Registered_Murals.nn", "Mural.isSig.dist")

## RUN REGRESSIONS
reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",                           
  dependentVariable = "countNarco",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countNarco, Prediction, geometry)

```

```{r map of errors by random k fold and spatial cross validation}

reg.ss.spatialCV <-
  reg.ss.spatialCV %>%
  mutate(
         countNarco.Error = Prediction - countNarco,
         countNarco.AbsError = abs(Prediction - countNarco),
         countNarco.APE = (abs(Prediction - countNarco)) / countNarco)

 ggplot(reg.ss.spatialCV)+
  geom_sf(aes(fill = countNarco.Error))+
  scale_fill_gradient(low = "black", high = "yellow", name = "Error of Predicted Narcotic Arrests")+
  mapTheme()

 ggplot(reg.ss.spatialCV)+
  geom_sf(aes(fill = countNarco.AbsError))+
  scale_fill_gradient(low = "black", high = "red", name = "Absolute Error of Predicted Narcotic Arrests")+
  mapTheme()

 ggplot(reg.ss.spatialCV)+
  geom_sf(aes(fill = countNarco.APE))+
  scale_fill_gradient(low = "black", high = "blue", name = "Absolute Percentage Error of Predicted Narcotic Arrests")+
  mapTheme()
# Gray represents infinitely off 

```


# A table of MAE and standard deviation MAE by regression.

```{r MAE and MAE Standard Deviation}

reg.ss.spatialCV_nogeom <- reg.ss.spatialCV %>%
  st_drop_geometry() %>%
  summarise(MAE = mean(countNarco.AbsError),
            MAESTD = sd(countNarco.AbsError)) %>%
  kbl(col.name=c('Mean Absolute Error','Mean Absolute Error Standard Deviation')) %>%
  kable_classic()

reg.ss.spatialCV_nogeom
```




# The map comparing kernel density to risk predictions for the next year’s crime.

## Get 2018 crime data

Let's see how our model performed relative to KD on the following year's data.

```{r}
narcs18 <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy") %>% 
  filter(Primary.Type == "NARCOTICS") %>%
  mutate(x = gsub("[()]", "", Location)) %>%
  separate(x,into= c("Y","X"), sep=",") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  na.omit %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271') %>% 
  distinct() %>%
  .[fishnet,]
```

```{r}
narc_ppp <- as.ppp(st_coordinates(drugArrest), W = st_bbox(final_net))
narc_KD.1000 <- spatstat.explore::density.ppp(narc_ppp, 1000)
narc_KD.1500 <- spatstat.explore::density.ppp(narc_ppp, 1500)
narc_KD.2000 <- spatstat.explore::density.ppp(narc_ppp, 2000)
narc_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(narc_KD.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(narc_KD.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(narc_KD.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

narc_KD.df$Legend <- factor(narc_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

narc_KDE_sum <- as.data.frame(narc_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(narc_KDE_sum$value, 
                             n = 5, "fisher")
narc_KDE_sf <- narc_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(narcs18) %>% mutate(countNarco = 1), ., sum) %>%
    mutate(countNarco = replace_na(countNarco, 0))) %>%
  dplyr::select(label, Risk_Category, countNarco)
```

Note that this is different from the book, where we pull a model out of a list of models we've created. For your homework, you'll be creating multiple models.

```{r}
ml_breaks <- classIntervals(reg.ss.spatialCV$Prediction, 
                             n = 5, "fisher")
narc_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(narcs18) %>% mutate(countNarco = 1), ., sum) %>%
      mutate(countNarco = replace_na(countNarco, 0))) %>%
  dplyr::select(label,Risk_Category, countNarco)
```

We don't do quite as well because we don't have very many features, but still pretty good.

## Kernel Density to Predict Next Year's Narcotic Arrests

```{r}
rbind(narc_KDE_sf, narc_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(narcs18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 narcotic risk predictions; 2018 narcotic arrests") +
    mapTheme(title_size = 14)
```

# Bar Plot Comparison of 2017 vs 2018

```{r}
rbind(narc_KDE_sf, narc_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countNarco = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countNarco / sum(countNarco)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2018 narco",
           y = "% of Test Set Narcotic Arrests (per model)",
           x = "Risk Category") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

# A table of raw errors by race context for a random k-fold vs. spatial cross validation regression.

```{r}

joined_data <- st_join(reg.ss.spatialCV, tracts17, join = st_intersects)
narc_risk_sf2 <- narc_risk_sf %>%
  dplyr::select("Risk_Category")

joined_data <- st_join(joined_data, narc_risk_sf2, join = st_intersects) %>% 
  dplyr::filter(countNarco.AbsError > 0.01)


```



```{r}

joined_data %>%
  st_centroid() %>%
  na.omit() %>%
  st_drop_geometry() %>%
  group_by(raceContext, Risk_Category) %>%
  summarize(mean.MAE = mean(countNarco.AbsError)) %>%
  spread(Risk_Category, mean.MAE) %>%
  mutate(across(everything(), function(x) ifelse(is.numeric(x), round(x, 2), x))) %>%
  kable(caption = "Mean Error by neighborhood racial context") %>%
  kable_styling("striped", full_width = F)



```

# Two Paragraphs of Why or why not recommend the algorithm. 




